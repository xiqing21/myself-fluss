我们借用 nexmark 数据集[8] 中 q20 的 query，略微修改后，作为本次实战的样例代码。-- 获取包含相应拍卖信息的出价表INSERT INTO nexmark_q20SELECT    auction, bidder, price, channel, url, B.`dateTime`, B.extra,    itemName, description, initialBid, reserve, A.`dateTime`, expires, seller, category, A.extraFROM    bid AS B INNER JOIN auction AS A on B.auction = A.id;-- WHERE A.category = 10;方式一：使用 Docker 环境测试1. 环境准备（1）类 Unix 操作系统，如 Linux、Mac OS X（2）内存建议至少 4 GB，磁盘建议至少 4 GB2. 下载 Docker 镜像在命令行中，运行如下命令安装 Docker 测试镜像。docker pull xuyangzzz/delta_join_example:1.0运行如下命令运行该测试镜像，进入测试 docker container 的命令行。docker run -p 8081:8081 -p 9123:9123 --name delta_join_example -it xuyangzzz/delta_join_example:1.0 bash3. 运行任务 SQL# 运行 flink 和 fluss 集群./start-flink-fluss.sh
# 创建相关表和 delta join 作业./create-tables-and-run-delta-join.sh此时，在宿主机 localhost:8081（或其他绑定的端口）即可查看 Flink UI 界面，可以看到此时 Delta Join 作业正在运行。4. 插入数据到源表在测试 docker container 中执行下面的命令，为源表插入数据。# 在源表插入数据./insert-data.sh5. 观察 Delta Join 作业在宿主机 localhost:8081（或其他绑定的端口）的 flink-ui 界面，就可以看到 Delta Join 作业在正常消费数据了。方式二：手工搭建环境测试1. 环境准备（1）运行环境a. 类 Unix 操作系统，如 Linux、Mac OS Xb. 内存建议至少 4 GB，磁盘建议至少 4 GBc. Java 11 及以上版本，且将环境变量 JAVA_HOME设置为 Java 的安装目录（2）准备 Apache Flink 计算引擎a. 下载在 Apache Flink 官方下载网站[9] 下载最新的 Flink 2.2.0 版本，并解压。b. 修改相关配置修改 ./conf/config.yaml 文件，将 TaskManager numberOfTaskSlots 设置成 4 （默认为1）（3）准备 Apache Fluss 流存储引擎在 Apache Fluss 官方下载网站[9] 分别下载 Fluss 0.8 版本（并解压）和适配 Apahce Flink 2.1 的连接器。（4）准备 Nexmark 源数据生成器下载 Nexmark 项目[10] master 分支，在该项目根目录下，用 maven-3.8.6 版本执行以下的 maven 命令mvn clean install -DskipTests=true在"./nexmark-flink/target/" 文件夹下，将会生成 nexmark-flink-0.3-SNAPSHOT.jar 文件2. 服务启动（1）启动 Flink 将 Fluss 适配 Flink 2.1 的连接器，以及 Nexmark 项目生成的 nexmark-flink-0.3-SNAPSHOT.jar 文件，放入 Flink 目录的 ./lib 目录下。参考 Flink 本地模式安装文档[11]，在 Flink 目录中，执行下面的语句，启动本地 Standalone 集群。## 请确保在 Flink 目录下执行该语句./bin/start-cluster.sh检查 http://localhost:8081/#/overview 界面是否可正常访问。（2）启动 Fluss参考 Fluss 部署 Local Cluster 文档[12]，在 Fluss 目录下，执行下面的语句，启动本地集群。## 请确保在 Fluss 目录下执行该语句./bin/local-cluster.sh start3. 运行任务 SQL（1）创建 Fluss 表将下面的 SQL 代码保存为“prepare_table.sql”文件，其中定义了 2 张源表和 1 张结果表。CREATE CATALOG fluss_catalogWITH (    'type'='fluss'    ,'bootstrap.servers'='localhost:9123');
USE CATALOG fluss_catalog;
CREATE DATABASE IF NOT EXISTS my_db;
USE my_db;
-- 创建左侧源表CREATE TABLE IF NOT EXISTS fluss_catalog.my_db.bid(    auction     BIGINT    ,bidder     BIGINT    ,price      BIGINT    ,channel    VARCHAR    ,url        VARCHAR    ,`dateTime` TIMESTAMP(3)    ,extra      VARCHAR    ,PRIMARY KEY (auction, bidder) NOT ENFORCED)WITH (-- fluss prefix lookup key，可用于 index    'bucket.key'='auction'-- Flink 2.2 中，delta join 仅支持消费不带 delete 操作的 cdc 源表    ,'table.delete.behavior'='IGNORE');
-- 创建右侧源表CREATE TABLE IF NOT EXISTS fluss_catalog.my_db.auction(    id           BIGINT    ,itemName    VARCHAR    ,description VARCHAR    ,initialBid  BIGINT    ,reserve     BIGINT    ,`dateTime`  TIMESTAMP(3)    ,expires     TIMESTAMP(3)    ,seller      BIGINT    ,category    BIGINT    ,extra       VARCHAR    ,PRIMARY KEY (id) NOT ENFORCED)WITH (-- Flink 2.2 中，delta join 仅支持消费不带 delete 操作的 cdc 源表    'table.delete.behavior'='IGNORE');
-- 创建 delta join 写入的结果表CREATE TABLE IF NOT EXISTS fluss_catalog.my_db.delta_join_sink(    auction           BIGINT    ,bidder           BIGINT    ,price            BIGINT    ,channel          VARCHAR    ,url              VARCHAR    ,bid_dateTime     TIMESTAMP(3)    ,bid_extra        VARCHAR    ,itemName         VARCHAR    ,description      VARCHAR    ,initialBid       BIGINT    ,reserve          BIGINT    ,auction_dateTime TIMESTAMP(3)    ,expires          TIMESTAMP(3)    ,seller           BIGINT    ,category         BIGINT    ,auction_extra    VARCHAR    ,PRIMARY KEY (auction, bidder) NOT ENFORCED);在 Flink 目录下，执行下面的语句，创建持久化的表。## 请确保在 Flink 目录下执行该语句## 注意：请将 ${your_path} 替换为 prepare_table.sql 实际所在的目录./bin/sql-client.sh -f ${your_path}/prepare_table.sql（2）启动 Delta Join 作业将下面的 SQL 代码保存为“run_delta_join.sql”文件，其中包含了可转化为 delta join 的 q20 变体查询。CREATE CATALOG fluss_catalogWITH (    'type'='fluss'    ,'bootstrap.servers'='localhost:9123');
USE CATALOG fluss_catalog;
USE my_db;
INSERT INTO delta_join_sinkSELECT    auction    ,bidder    ,price    ,channel    ,url    ,B.`dateTime`    ,B.extra    ,itemName    ,description    ,initialBid    ,reserve    ,A.`dateTime`    ,expires    ,seller    ,category    ,A.extraFROM bid AS BINNER JOIN auction AS AON B.auction = A.id;在 Flink 目录下，执行下面的语句，启动 delta join 作业。## 请确保在 Flink 目录下执行该语句## 注意：请将 ${your_path} 替换为 run_delta_join.sql 实际所在的目录./bin/sql-client.sh -f ${your_path}/run_delta_join.sql在 Flink UI 上，我们可以看到 Delta Join 作业正常跑起来了。4. 插入数据到源表将下面的 SQL 代码保存为“insert_data.sql”文件，其中包含了向两张源表灌入 Nexmark 数据源产生模拟数据的作业。CREATE CATALOG fluss_catalogWITH (    'type' = 'fluss'    ,'bootstrap.servers' = 'localhost:9123');
USE CATALOG fluss_catalog;
USE my_db;
-- nexmark 模拟数据源CREATE TEMPORARY TABLE datagen(    event_type  int    ,person ROW<        id BIGINT        ,name VARCHAR        ,emailAddress VARCHAR        ,creditCard VARCHAR        ,city VARCHAR        ,state VARCHAR        ,`dateTime` TIMESTAMP(3)        ,extra VARCHAR >    ,auction ROW<        id BIGINT        ,itemName VARCHAR        ,description VARCHAR        ,initialBid BIGINT        ,reserve BIGINT        ,`dateTime` TIMESTAMP(3)        ,expires TIMESTAMP(3)        ,seller BIGINT        ,category BIGINT        ,extra VARCHAR >    ,bid ROW<        auction BIGINT        ,bidder BIGINT        ,price BIGINT        ,channel VARCHAR        ,url VARCHAR        ,`dateTime` TIMESTAMP(3)        ,extra VARCHAR >    ,`dateTime` AS         CASE          WHEN event_type = 0 THEN person.`dateTime`          WHEN event_type = 1 THEN auction.`dateTime`          ELSE bid.`dateTime`        END    ,WATERMARK FOR `dateTime` AS `dateTime` - INTERVAL '4' SECOND)WITH (    'connector' = 'nexmark'    -- 下面两个参数为每秒数据生成速度    ,'first-event.rate' = '1000'    ,'next-event.rate' = '1000'    -- 生成的数据总条数，过大可能导致 OOM    ,'events.num' = '100000'    -- 下面三个参数为 Bid/Auction/Persion 三个数据的生成占比    ,'person.proportion' = '2'    ,'auction.proportion' = '24'    ,'bid.proportion' = '24');
CREATE TEMPORARY VIEW auction_viewAS SELECT    auction.id    ,auction.itemName    ,auction.description    ,auction.initialBid    ,auction.reserve    ,`dateTime`    ,auction.expires    ,auction.seller    ,auction.category    ,auction.extraFROM datagenWHERE event_type = 1;
CREATE TEMPORARY VIEW bid_viewAS SELECT    bid.auction    ,bid.bidder    ,bid.price    ,bid.channel    ,bid.url    ,`dateTime`    ,bid.extraFROM datagenWHERE event_type = 2;
INSERT INTO bidSELECT    *FROM bid_view;
INSERT INTO auctionSELECT    *FROM auction_view;在 Flink 目录下，执行下面的语句，启动两个将 nexmark 模拟数据写入源表的作业。## 请确保在 Flink 目录下执行该语句## 注意：请将 ${your_path} 替换为 insert_data.sql 实际所在的目录./bin/sql-client.sh -f ${your_path}/insert_data.sql5. 观察 Delta Join 作业重新点击 Flink UI 上的 Delta Join 作业，可以看到 Delta Join 作业正常在消费数据了。