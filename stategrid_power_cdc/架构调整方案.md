# Flink 2.2.0 架构调整方案

由于 Flink 2.2.0 不支持 JDBC Source 和 CDC，建议使用以下架构：

## 推荐架构：Kafka 作为中间层

```
PostgreSQL (Source)
      ↓
Debezium CDC (独立进程)
      ↓
    Apache Kafka
      ↓
   Flink 2.2.0
      ↓
    Fluss 分层 (ODS→DWD→DWS→ADS)
      ↓
  PostgreSQL (Sink via JDBC 或其他方式)
```

### 实施步骤

#### 1. 安装 Debezium + PostgreSQL Connector
Debezium 可以独立运行，捕获 PostgreSQL CDC 并写入 Kafka

```bash
# Debezium 通过 Kafka Connect 部署
# 配置 PostgreSQL CDC 连接器
```

#### 2. 安装 Kafka
```bash
# Docker 启动 Kafka
docker run -d --name kafka \
  -p 9092:9092 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  confluentinc/cp-kafka:latest
```

#### 3. Debezium CDC 配置
```json
{
  "name": "stategrid-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "localhost",
    "database.port": "5432",
    "database.user": "postgres",
    "database.password": "postgres",
    "database.dbname": "stategrid_db",
    "topic.prefix": "stategrid",
    "plugin.name": "wal2json",
    "slot.name": "debezium_stategrid"
  }
}
```

#### 4. Flink SQL 使用 Kafka Source
```sql
-- 修改 01-run-ods-cdc.sql
CREATE TABLE pg_power_user (
    user_id BIGINT,
    user_name STRING,
    usage_type STRING,
    region_id INT,
    region_name STRING,
    address STRING,
    phone STRING,
    create_time TIMESTAMP(3),
    update_time TIMESTAMP(3),
    PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
    'connector' = 'kafka',
    'topic' = 'stategrid.public.power_user',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'flink-stategrid',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'debezium-json',
    'debezium-json.schema-include' = 'true'
);

CREATE TABLE pg_power_consumption (
    consumption_id BIGINT,
    user_id BIGINT,
    consumption_amount DECIMAL(10, 2),
    consumption_cost DECIMAL(10, 2),
    consumption_date TIMESTAMP(3),
    meter_reading_before DECIMAL(10, 2),
    meter_reading_after DECIMAL(10, 2),
    remark STRING,
    PRIMARY KEY (consumption_id) NOT ENFORCED
) WITH (
    'connector' = 'kafka',
    'topic' = 'stategrid.public.power_consumption',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'flink-stategrid',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'debezium-json',
    'debezium-json.schema-include' = 'true'
);
```

---

## 方案 2：使用 File Source（用于测试）

如果只是测试，可以先将 PostgreSQL 数据导出为 JSON 文件：

```sql
-- 将 PostgreSQL 数据导出为 JSON
COPY (
    SELECT row_to_json(t)
    FROM (
        SELECT * FROM power_user
    ) t
) TO '/tmp/power_user.json';
```

```sql
-- Flink SQL 使用 File Source
CREATE TABLE pg_power_user (
    user_id BIGINT,
    user_name STRING,
    usage_type STRING,
    region_id INT,
    region_name STRING,
    address STRING,
    phone STRING,
    create_time TIMESTAMP(3),
    update_time TIMESTAMP(3),
    PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
    'connector' = 'filesystem',
    'path' = 'file:///tmp/power_user.json',
    'format' = 'json'
);
```

---

## 方案 3：降级到 Flink 1.19.x（最简单）

如果环境允许，降级到 Flink 1.19.1：

```bash
# 停止 Flink 2.2.0
# 下载并启动 Flink 1.19.1
wget https://dlcdn.apache.org/flink/flink-1.19.1/flink-1.19.1-bin-scala_2.12.tgz
tar -xzf flink-1.19.1-bin-scala_2.12.tgz
cd flink-1.19.1
# 使用原有的 CDC 3.5.0 配置
```

---

## API 选择建议

### Table API vs DataStream API

**推荐使用 Table API & SQL**，原因：

1. ✅ 你的项目本身就是 SQL 脚本（`01-run-ods-cdc.sql`）
2. ✅ Table API 更适合数仓分层（ODS/DWD/DWS/ADS）
3. ✅ 声明式 SQL 更易维护和调试
4. ✅ Flink SQL 对 CDC、Format、Watermark 支持更好

**DataStream API 适用场景：**
- 需要复杂的状态管理逻辑
- 自定义算子实现特殊需求
- 需要细粒度的性能控制

---

## PostgreSQL Sink 支持

**Table API & SQL：**
- ❌ Flink 2.2.0 没有 JDBC Sink connector
- ✅ 但可以通过 `print` connector 输出到控制台验证
- ✅ 或通过 `kafka` connector 写入 Kafka

**DataStream API：**
- ✅ 可以使用 JDBC 写入（但需要手动实现）
- ✅ 或使用 Kafka 作为中间层

---

## 我的最终建议

**优先级排序：**

1. **方案 1（Kafka + Debezium）** - 生产环境推荐
   - 完整的 CDC 能力
   - Kafka 提供数据缓冲和解耦
   - Flink 2.2.0 原生支持 Kafka

2. **方案 3（降级 Flink）** - 最快解决问题
   - 如果环境允许，这是最简单的方案
   - 直接使用原有的 CDC 3.5.0 配置

3. **方案 2（File Source）** - 仅限测试
   - 适合验证 Flink SQL 逻辑
   - 不支持实时更新

---

## 下一步

你想选择哪个方案？
- 方案 1：我可以帮你配置 Kafka + Debezium
- 方案 3：我可以帮你部署 Flink 1.19.1
- 方案 2：我可以帮你导出数据并配置 File Source
