# Fluss 分层表配置注意点

## 1. 多个 INSERT 语句必须使用 EXECUTE STATEMENT SET

### 核心规则

**在 Flink SQL 中执行多个 `INSERT` 语句时，必须使用 `EXECUTE STATEMENT SET BEGIN ... END` 包裹，否则 Flink 只会执行第一条语句。**

### 错误示例

```sql
-- ❌ 错误：没有使用 EXECUTE STATEMENT SET 包裹
SET 'pipeline.name' = 'My Job';

INSERT INTO table1 SELECT ... FROM source1;
INSERT INTO table2 SELECT ... FROM source2;  -- 这条永远不会被执行！
```

**问题现象：**
- Flink SQL Client 执行后返回成功
- 但 Flink 作业列表中找不到对应作业
- 只有第一个 INSERT 被执行（甚至可能只是执行了 CREATE CATALOG）

### 正确写法

```sql
-- ✅ 正确：使用 EXECUTE STATEMENT SET 包裹
SET 'pipeline.name' = 'My Job';

EXECUTE STATEMENT SET
BEGIN
INSERT INTO table1 SELECT ... FROM source1;
INSERT INTO table2 SELECT ... FROM source2;
END;
```

### 为什么必须这样？

1. **Flink SQL Client 执行机制**：
   - SQL Client 默认只会执行文件中的第一条语句
   - 多条语句需要明确告诉 Flink 一起提交

2. **EXECUTE STATEMENT SET 作用**：
   - 将多个 INSERT 语句合并成一个 Flink 作业
   - 所有 INSERT 共享同一个 Checkpoint 和状态
   - 优化性能，减少作业数量

3. **适用场景**：
   - 从一个源表写入多个目标表
   - 多个 sink 操作需要原子性保证
   - DWS/ADS 层的多路输出

### 项目示例

查看 `03-run-dws-aggregate.sql`：

```sql
-- ✅ 正确示例
SET 'pipeline.name' = 'StateGrid CDC: DWS Layer (Aggregate)';

-- 创建 Catalog
CREATE CATALOG fluss_catalog WITH ('type' = 'fluss', ...);
USE CATALOG fluss_catalog;
USE stategrid_db;

-- 使用 EXECUTE STATEMENT SET 包裹多个 INSERT
EXECUTE STATEMENT SET
BEGIN
    -- 地区日汇总
    INSERT INTO dws_region_daily_stats
    SELECT ... FROM dwd_power_consumption_detail GROUP BY ...;

    -- 用户用电排名
    INSERT INTO dws_user_ranking
    SELECT ... FROM dwd_power_consumption_detail GROUP BY ...;
END;
```

### 调试建议

1. **检查 SQL 文件**：
   - 搜索文件中的 `INSERT INTO` 关键字
   - 确认是否有多个 INSERT 语句
   - 如果有，必须用 `EXECUTE STATEMENT SET BEGIN ... END` 包裹

2. **验证作业是否提交**：
   - 执行 SQL 后立即检查 Flink Web UI
   - 或使用 `check-jobs.sh` 脚本验证
   - 如果没有看到新作业，很可能是缺少 `EXECUTE STATEMENT SET`

3. **单 INSERT 语句的情况**：
   - 如果只有一个 INSERT 语句，不需要 `EXECUTE STATEMENT SET`
   - 但建议统一使用，保持代码一致性

---

## 2. 分桶键配置规则

### 核心规则

**Fluss 主键表（含 `PRIMARY KEY` 定义）的分桶键（`bucket.key`）必须是主键的子集。**

## 错误示例

```sql
CREATE TABLE ods_power_consumption (
    consumption_id BIGINT,
    user_id BIGINT,
    ...
    PRIMARY KEY (consumption_id) NOT ENFORCED  -- 主键是 consumption_id
) WITH (
    'bucket.key' = 'user_id',  -- ❌ 错误：user_id 不是主键的子集
    'table.delete.behavior' = 'IGNORE'
);
```

**错误信息：**
```
Bucket keys must be a subset of primary keys excluding partition keys.
The primary keys are [consumption_id], partition keys are [],
but user-defined bucket keys are [user_id].
```

## 正确配置

### 方案1：使用主键作为分桶键（推荐）

```sql
CREATE TABLE ods_power_consumption (
    consumption_id BIGINT,
    user_id BIGINT,
    ...
    PRIMARY KEY (consumption_id) NOT ENFORCED
) WITH (
    'bucket.key' = 'consumption_id',  -- ✅ 正确：使用主键字段
    'table.delete.behavior' = 'IGNORE'
);
```

### 方案2：使用复合主键

如果业务需要按 `user_id` 分桶，可以修改主键为复合主键：

```sql
CREATE TABLE ods_power_consumption (
    consumption_id BIGINT,
    user_id BIGINT,
    ...
    PRIMARY KEY (user_id, consumption_id) NOT ENFORCED  -- 复合主键
) WITH (
    'bucket.key' = 'user_id',  -- ✅ 正确：user_id 是主键的子集
    'table.delete.behavior' = 'IGNORE'
);
```

## 本项目配置汇总

| 表名 | 主键 | 分桶键 | 状态 |
|------|------|--------|------|
| `ods_power_user` | `user_id` | `user_id` | ✅ 正确 |
| `ods_power_consumption` | `consumption_id` | `consumption_id` | ✅ 已修复 |
| `dwd_power_consumption_detail` | `consumption_id` | `consumption_id` | ✅ 已修复 |
| `dws_region_daily_stats` | `region_id, stat_date` | `region_id` | ✅ 正确 |
| `dws_user_ranking` | `user_id, stat_date` | `user_id` | ✅ 正确 |
| `ads_power_dashboard` | `dashboard_id, stat_date, region_id` | `dashboard_id` | ✅ 已修复 |

## 为什么需要这个限制？

1. **数据一致性**：同一主键的更新/删除操作必须路由到同一个桶，避免数据分散到多个桶导致不一致。
2. **性能优化**：按主键分桶可以更高效地进行主键查找和更新操作。
3. **CDC 语义支持**：Flink CDC 需要主键来支持 upsert/delete 语义。

## 常见场景

### 场景1：CDC 同步表
- **推荐**：使用源表主键作为分桶键
- **原因**：CDC 场景下，主键是数据的唯一标识，按主键分桶最合理

### 场景2：聚合统计表（DWS层）
- **推荐**：使用维度字段作为分桶键
- **示例**：地区汇总表使用 `region_id`，用户排名表使用 `user_id`
- **原因**：聚合查询通常按维度字段过滤，按维度分桶可以减少数据扫描

### 场景3：关联表（DWD层）
- **推荐**：使用主表主键或关联键作为分桶键
- **示例**：消费明细表使用 `consumption_id`
- **原因**：DWD 层数据通常以 CDC 同步为主键，需要支持 upsert

## 调试建议

1. **检查主键定义**：确保 `PRIMARY KEY` 正确定义
2. **检查分桶键**：确认 `bucket.key` 是主键的子集
3. **查看错误信息**：Fluss 会明确提示主键和分桶键的不匹配

---

## 3. Fluss Catalog 表类型限制

## 参考资料

- Flink 2.2.0 文档：https://nightlies.apache.org/flink/flink-docs-release-2.2/
- Fluss 0.8.0 文档：https://fluss.apache.org/

---

## 4. Fluss Catalog 表类型限制

### 问题描述

Fluss Catalog 有一个限制：**仅支持"Fluss 类型表"**，不允许直接创建带有第三方 Connector（如 JDBC、Kafka）的"持久表"（PERMANENT TABLE）。

### 错误示例

```sql
USE CATALOG fluss_catalog;

-- ❌ 错误：Fluss Catalog 不支持 JDBC 持久表
CREATE TABLE IF NOT EXISTS ads_power_dashboard_sink (
    ...
) WITH (
    'connector' = 'jdbc',  -- 第三方 Connector
    ...
);
```

**错误信息：**
```
Failed to create table "ads_power_dashboard_sink" in catalog "fluss_catalog".
Fluss catalog only supports Fluss tables.
```

### 解决方案

#### 方案1：使用 TEMPORARY 临时表（当前采用方案）

```sql
USE CATALOG fluss_catalog;

-- ✅ 正确：添加 TEMPORARY 关键字
CREATE TEMPORARY TABLE IF NOT EXISTS ads_power_dashboard_sink (
    dashboard_id STRING,
    stat_date DATE,
    ...
    PRIMARY KEY (dashboard_id, stat_date, region_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    ...
);
```

**TEMPORARY 表特性：**
- 仅在当前 Flink Session 内有效
- Session 关闭后自动消失
- 允许使用任意 Flink 支持的 Connector
- 不受 Fluss Catalog 对"持久表"的限制

**适用场景：**
- 测试环境
- 临时任务
- 无需跨 Session 共享表定义

---

#### 方案2：切换到 Flink 默认 Catalog（推荐，无临时表限制）

Flink 默认 Catalog（`default_catalog` 或 `GenericInMemoryCatalog`）支持所有 Connector 类型的表。

```sql
-- 1. 切换到默认 Catalog
USE CATALOG default_catalog;
USE default_database;

-- 2. 创建持久表（无需 TEMPORARY）
CREATE TABLE IF NOT EXISTS ads_power_dashboard_sink (
    ...
    PRIMARY KEY (dashboard_id, stat_date, region_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    ...
);

-- 3. 从 Fluss 表读取并写入
INSERT INTO ads_power_dashboard_sink
SELECT * FROM fluss_catalog.stategrid_db.ads_power_dashboard;
```

**优势：**
- 表是持久表（PERMANENT），存储在 Catalog 中
- 多个 Flink Session 可共享访问
- 无需修改表定义，仅切换 Catalog 即可

**注意：**
- 默认 Catalog 是内存级别的，Flink 集群重启后表元数据会丢失
- 如需元数据持久化，可使用 Hive Catalog

---

#### 方案3：使用 Hive Catalog（元数据持久化）

如果需要「表元数据持久化」，可使用 Hive Catalog。

```sql
-- 1. 创建 Hive Catalog
CREATE CATALOG hive_catalog
WITH (
    'type' = 'hive',
    'hive-conf-dir' = '/path/to/hive/conf',
    'default-database' = 'default'
);

-- 2. 切换到 Hive Catalog
USE CATALOG hive_catalog;
USE default;

-- 3. 创建持久表
CREATE TABLE IF NOT EXISTS ads_power_dashboard_sink (
    ...
) WITH (
    'connector' = 'jdbc',
    ...
);
```

**优势：**
- 表元数据持久化到 Hive，集群重启后不丢失
- 支持多 Session 共享、跨集群访问

**注意：**
- 需要额外部署 Hive 环境
- 确保 Flink 版本与 Hive 版本兼容

---

#### 方案4：Fluss Catalog 外部表映射（需 Fluss 支持）

如果必须使用 Fluss Catalog，可检查 Fluss 是否支持「外部表映射」。

**操作逻辑：**
1. 在 Fluss Catalog 中创建外部表映射，配置 PostgreSQL 连接信息
2. 在 Flink SQL 中直接引用该外部表，无需指定 Connector

```sql
USE CATALOG fluss_catalog;

-- 假设已在 Fluss Catalog 中创建外部表映射
INSERT INTO ads_power_dashboard_sink
SELECT * FROM ads_power_dashboard;
```

**优势：**
- 符合 Fluss Catalog 使用规范
- 表元数据统一管理

**注意：**
- 依赖 Fluss Catalog 本身支持外部表映射功能

---

### 各方案对比

| 方案 | 核心特点 | 适用场景 |
|------|----------|----------|
| TEMPORARY 临时表 | 简单快捷，Session 级有效 | 测试环境、临时任务 |
| 切换默认 Catalog | 无需额外依赖，支持持久表（内存级） | 开发环境、临时生产任务 |
| Hive Catalog | 元数据持久化，多 Session 共享 | 正式生产环境 |
| Fluss 外部表映射 | 不脱离 Fluss Catalog，统一管理 | 公司强制使用 Fluss Catalog |

### 选择建议

- **开发/测试环境**：方案1（临时表）或方案2（默认 Catalog）
- **生产环境**：方案3（Hive Catalog），兼顾元数据持久化和兼容性
- **必须使用 Fluss Catalog**：方案4（需确认 Fluss 支持外部表映射）

---

## 5. 本项目配置说明

### 当前采用方案

本项目使用**方案1（TEMPORARY 临时表）** + **方案2（默认 Catalog 备选）**：

1. **Fluss 分层表**（ODS/DWD/DWS/ADS）：使用 Fluss Catalog
2. **PostgreSQL Sink 表**：使用 TEMPORARY 表在 Fluss Catalog 中创建

### 文件说明

- `create-fluss-tables.sql`：创建所有 Fluss 分层表 + TEMPORARY JDBC Sink 表
- `run-fluss-to-postgres.sql`：执行 Sink 作业，同时提供了默认 Catalog 的备选方案

### 切换到默认 Catalog（如需要）

如果需要持久化 Sink 表，修改 `run-fluss-to-postgres.sql`：

```sql
-- 取消注释方案2的代码
USE CATALOG default_catalog;
USE default_database;

-- 创建持久表
CREATE TABLE IF NOT EXISTS ads_power_dashboard_sink (...)
WITH ('connector' = 'jdbc', ...);

-- 执行插入
INSERT INTO ads_power_dashboard_sink
SELECT * FROM fluss_catalog.stategrid_db.ads_power_dashboard;
```




# 1. 启动 Flink 和 Fluss
cd /opt/data
./start-flink-fluss.sh

# 2. 初始化 PostgreSQL 和 CDC
cd /opt/data/stategrid_power_cdc
./init-postgres-and-cdc.sh

# 3. 创建 Fluss 分层表
./create-fluss-tables.sh

# 4. 启动所有数据处理作业
./run-all-jobs.sh

# 5. 安装 Python 依赖
pip install -r requirements.txt

# 6. 插入测试数据
python insert-test-data.py

# 7. 性能监控
python monitor_performance.py

# 8. CRUD 测试
python test_crud.py
